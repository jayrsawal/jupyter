{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "import pickle\n",
    "import codecs\n",
    "import string\n",
    "import psycopg2\n",
    "import re\n",
    "import multiprocessing\n",
    "import timeit\n",
    "import os\n",
    "import spacy\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "from lxml import etree\n",
    "from sqlalchemy import create_engine\n",
    "warnings.filterwarnings('ignore')\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config(filename='database.ini', section='postgresql'):\n",
    "    # create a parser\n",
    "    parser = ConfigParser()\n",
    "    # read config file\n",
    "    parser.read(filename)\n",
    " \n",
    "    # get section, default to postgresql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    " \n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect():\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # read connection parameters\n",
    "        params = config()\n",
    "        # connect to the PostgreSQL server\n",
    "        conn = psycopg2.connect(**params)\n",
    "        conn.autocommit = True\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        return conn\n",
    "conn = connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(sql, data=()):\n",
    "    try:\n",
    "        # read database configuration\n",
    "        params = config()\n",
    "        # connect to the PostgreSQL database\n",
    "        conn = psycopg2.connect(**params)\n",
    "        # create a new cursor\n",
    "        cur = conn.cursor()\n",
    "        # execute the UPDATE  statement\n",
    "        cur.execute(sql, data)\n",
    "        # get the number of updated rows\n",
    "        results = cur.fetchall()\n",
    "        # Commit the changes to the database\n",
    "        conn.commit()\n",
    "        # Close communication with the PostgreSQL database\n",
    "        cur.close()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "    return results\n",
    "\n",
    "def run_update(sql, data=(), is_insert=False):\n",
    "    try:\n",
    "        # read database configuration\n",
    "        params = config()\n",
    "        # connect to the PostgreSQL database\n",
    "        conn = psycopg2.connect(**params)\n",
    "        # create a new cursor\n",
    "        cur = conn.cursor()\n",
    "        # execute the UPDATE  statement\n",
    "        cur.execute(sql, data)\n",
    "        if is_insert:\n",
    "            updated_rows = cur.fetchone()[0]\n",
    "        else:\n",
    "            # get the number of updated rows\n",
    "            updated_rows = cur.rowcount\n",
    "        # Commit the changes to the database\n",
    "        conn.commit()\n",
    "        # Close communication with the PostgreSQL database\n",
    "        cur.close()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "    return updated_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(dir):\n",
    "    r = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for name in files:\n",
    "            if \"nxml\" in name: \n",
    "                r.append(os.path.join(root, name))\n",
    "    return r "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(root, path):\n",
    "    node = root.xpath(path)\n",
    "    if len(node) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return node[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_article(article_id, pmc, doi, filename, fauthor, authors):\n",
    "    sql = \"\"\"\n",
    "        update temp_article\n",
    "        set pmc=%s\n",
    "        , doi=%s\n",
    "        , filename=%s\n",
    "        , fauthor=%s\n",
    "        , authors=%s\n",
    "        where id=%s\n",
    "    \"\"\"\n",
    "    run_update(sql, (pmc,doi,filename,fauthor,\", \".join(authors),article_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_reference(pmid, pmc, doi, raw, source, title, fauthor, authors, year, reftype):\n",
    "    ref_id = pmid\n",
    "    id_col = \"pmid\"\n",
    "    if ref_id is None:\n",
    "        ref_id = pmc\n",
    "        id_col = \"pmc\"\n",
    "        if pmc is None:\n",
    "            ref_id = doi\n",
    "            id_col = \"doi\"\n",
    "            if doi is None:\n",
    "                ref_id = raw\n",
    "                id_col = \"raw\"\n",
    "                if raw is None:\n",
    "                    ref_id = title\n",
    "                    id_col = \"articletitle\"\n",
    "                    if title is None:\n",
    "                        ref_id = source\n",
    "                        id_col = \"articlesource\"\n",
    "                        if source is None:\n",
    "                            return None\n",
    "\n",
    "    # let's see if this is an internal article\n",
    "    articles = run_query(\"select id from temp_article where id=%s\",(ref_id,))\n",
    "    article_id = None\n",
    "    if len(articles) > 0:\n",
    "        article_id = articles[0][0]\n",
    "    # check if this reference already exists\n",
    "    ref = run_query(\"select id from reference where \" + id_col + \"=%s\", (ref_id,))\n",
    "    if len(ref) > 0:\n",
    "        return ref[0][0]\n",
    "    # new reference, let's save it then\n",
    "    return run_update(\"\"\"\n",
    "        insert into reference(pmid\n",
    "        ,pmc\n",
    "        ,doi\n",
    "        ,raw\n",
    "        ,articletitle\n",
    "        ,articlesource\n",
    "        ,fauthor\n",
    "        ,authors\n",
    "        ,articleyear\n",
    "        ,article_id\n",
    "        ,referencetype)\n",
    "        values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "        returning id;\n",
    "    \"\"\" , (pmid, pmc, doi, raw, title, source, fauthor, \", \".join(authors), re.sub(\"[^0-9]\", \"\", year), article_id, reftype)\n",
    "        , True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_citation(article_id, citation_id, reference_id):\n",
    "    return run_update(\"\"\"\n",
    "        insert into citation_ref(articleid, citationid, reference_id)\n",
    "        values(%s, %s, %s)\n",
    "        returning reference_id;\n",
    "    \"\"\" , (article_id, citation_id, reference_id)\n",
    "        , True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_list = list_files(\"./data/citations/datasets\")\n",
    "processed_files = run_query(\"select distinct filename from temp_article where filename is not null\")\n",
    "processed_files = [f[0] for f in processed_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(f, n):\n",
    "    root = etree.parse(f)\n",
    "    ref_list = root.xpath(\"//back/ref-list/ref\")\n",
    "    article_pmid = getText(root, \"//front/article-meta/article-id[@pub-id-type='pmid']\")\n",
    "    article_pmc = getText(root, \"//front/article-meta/article-id[@pub-id-type='pmc']\")\n",
    "    if article_pmc is not None and \"PMC\" not in article_pmc:\n",
    "        article_pmc = \"PMC\" + article_pmc\n",
    "        \n",
    "    # make sure we have an article id to update.. otherwise break\n",
    "    if article_pmid is None:\n",
    "        if article_pmc is not None:\n",
    "            article_pmid = article_pmc\n",
    "        else:\n",
    "            return\n",
    "    article_doi = getText(root, \"//front/article-meta/article-id[@pub-id-type='doi']\")\n",
    "    article_filename = f.split(\"/\")[-1]\n",
    "    \n",
    "    article_authors = []\n",
    "    authors = root.xpath(\"//front/article-meta/contrib-group/contrib[@contrib-type='author']/name\")\n",
    "    for author in authors:\n",
    "        fname = getText(author, \".//given-names\")\n",
    "        surname = getText(author, \".//surname\")\n",
    "        if fname is not None:\n",
    "            fname = re.sub(r\"[^A-Za-z]+\", '', fname)\n",
    "        else:\n",
    "            fname = \"\"\n",
    "        article_authors.append((fname + \" \" + surname).strip())\n",
    "\n",
    "    article_fauthor = None\n",
    "    if len(article_authors) > 0:\n",
    "        article_fauthor = article_authors[0]\n",
    "    \n",
    "    # update article table\n",
    "    update_article(article_pmid, article_pmc, article_doi, article_filename, article_fauthor, article_authors)\n",
    "    \n",
    "    # now update our citation references\n",
    "    for ref in ref_list:\n",
    "        elem = ref.xpath(\".//element-citation\")\n",
    "        reftype = \"mixed\"\n",
    "        if len(elem) > 0:\n",
    "            reftype = elem[0].get(\"publication-type\")\n",
    "        title = getText(ref, \".//element-citation/article-title\")\n",
    "        year = getText(ref, \".//element-citation/year\")\n",
    "        source = getText(ref, \".//element-citation/source\")\n",
    "        if title is None:\n",
    "            title = source\n",
    "            if title is None:\n",
    "                continue\n",
    "        if year is None:\n",
    "            year = \"\"\n",
    "            \n",
    "        # skip papers we've already processed\n",
    "        title_year = title+\"//\"+year\n",
    "        \n",
    "        # collect missing meta data\n",
    "        pmid = getText(ref, \".//pub-id[@pub-id-type='pmid']\")\n",
    "        pmc = getText(ref, \".//pub-id[@pub-id-type='pmc']\")\n",
    "        doi = getText(ref, \".//pub-id[@pub-id-type='doi']\")\n",
    "        raw = getText(ref, \".//mixed-citation\")\n",
    "        source = getText(ref, \".//source\")\n",
    "        title = getText(ref, \".//article-title\")\n",
    "        year = getText(ref, \".//year\")\n",
    "        # get authors\n",
    "        authors = root.xpath(\"//front/article-meta/contrib-group/contrib[@contrib-type='author']/name\")\n",
    "        ref_authors = []\n",
    "        for author in authors:\n",
    "            fname = getText(author, \".//given-names\")\n",
    "            surname = getText(author, \".//surname\")\n",
    "            if fname is not None:\n",
    "                fname = re.sub(r\"[^A-Za-z]+\", '', fname)\n",
    "            else:\n",
    "                fname = \"\"\n",
    "            ref_authors.append((fname + \" \" + surname).strip())\n",
    "        # save first author for easy access\n",
    "        fauthor = None\n",
    "        if len(ref_authors) > 0:\n",
    "            fauthor = ref_authors[0]\n",
    "        \n",
    "        # update table with new meta data\n",
    "        ref_id = update_reference(pmid, pmc, doi, raw, source, title, fauthor, ref_authors, year, reftype)\n",
    "        update_citation(article_pmid, ref.get(\"id\"), ref_id)\n",
    "    \n",
    "    n.value += 1\n",
    "    if n.value % 10000 == 0:\n",
    "        print(n.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start_time = timeit.default_timer()\n",
    "# pool = multiprocessing.Pool(processes=multiprocessing.cpu_count()-1 or 1)\n",
    "# manager = multiprocessing.Manager()\n",
    "# n = manager.Value('i',0)\n",
    "# for f in file_list:\n",
    "#     pool.apply_async(parse_file, args=(f, n))\n",
    "# pool.close()\n",
    "# pool.join()\n",
    "# elapsed = timeit.default_timer() - start_time\n",
    "# print(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['ner', 'parser'])\n",
    "EMBEDDINGS_WORD2VEC_MODEL_FILE = \"word2vec.model\"\n",
    "\n",
    "\n",
    "def text_to_tokens(text: str) -> [str]:\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc \\\n",
    "            if token.lemma_ not in nlp.Defaults.stop_words \\\n",
    "            and len(token.lemma_) > 2 \\\n",
    "            and token.lemma_.isalpha() \\\n",
    "           ]\n",
    "\n",
    "\n",
    "def tokens_generator():\n",
    "    for qa in qa_generator():\n",
    "        yield text_to_tokens(qa[2])\n",
    "\n",
    "\n",
    "def qa_generator():\n",
    "    \"\"\"Generator which returns Q/A tuples.\n",
    "    Each tuple has an user, a group, a timestamp, the tokens and raw text.\n",
    "    Consecutive messages from the same user in the same group are aggregated.\n",
    "    \"\"\"\n",
    "    rows = run_query(\"select id, articletitle, papertext from article limit 100\")\n",
    "    for row in rows:\n",
    "        yield row\n",
    "\n",
    "\n",
    "class SentencesIterator():\n",
    "    def __init__(self, generator_function):\n",
    "        self.generator_function = generator_function\n",
    "        self.generator = self.generator_function()\n",
    "\n",
    "    def __iter__(self):\n",
    "        # reset the generator\n",
    "        self.generator = self.generator_function()\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        result = next(self.generator)\n",
    "        if result is None:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "        \n",
    "def get_model():\n",
    "    model = Word2Vec.load(EMBEDDINGS_WORD2VEC_MODEL_FILE)\n",
    "    return model\n",
    "        \n",
    "\n",
    "def build_model():\n",
    "    print('Connecting to the database...')\n",
    "    sentences = SentencesIterator(tokens_generator)\n",
    "    print('Calculating the embeddings...')\n",
    "    model = Word2Vec(sentences, min_count=10)\n",
    "    print('Saving the model...')\n",
    "    model.save(EMBEDDINGS_WORD2VEC_MODEL_FILE)\n",
    "    print('Word2Vec Model saved.')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the database...\n",
      "Calculating the embeddings...\n",
      "Saving the model...\n",
      "Word2Vec Model saved.\n",
      "Time to build model: 84.7 s\n"
     ]
    }
   ],
   "source": [
    "# t = time()\n",
    "# w2v = build_model()\n",
    "# print('Time to build model: {} s'.format(round((time() - t), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comparable', 0.8415109515190125),\n",
       " ('consistent', 0.731269121170044),\n",
       " ('identical', 0.6702821254730225),\n",
       " ('concordant', 0.657517671585083),\n",
       " ('contrary', 0.6442310810089111),\n",
       " ('surprising', 0.6369888782501221),\n",
       " ('similarly', 0.6329345107078552),\n",
       " ('differ', 0.6311222910881042),\n",
       " ('indistinguishable', 0.630509614944458),\n",
       " ('different', 0.6298468112945557)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(\"similar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
